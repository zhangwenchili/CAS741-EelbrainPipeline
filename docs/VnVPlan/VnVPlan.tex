\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{listings}

\input{../Comments.text}
\input{../Common.text}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}








%%%%% 1 Plan %%%%%
\section*{Revision History}


\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
    Feb 8, 2026 & 1.0 & Initial draft \\
\bottomrule
\end{tabularx}
~\\


\newpage


\tableofcontents
% \listoftables
% \listoffigures


\newpage








%%%%% 2 Plan %%%%%
\section{Symbols, Abbreviations, and Acronyms}


Identical to the same section in SRS document \citep{SRS}.


\pagenumbering{arabic}


\newpage


This document presents the Verification and Validation (V\&V) plan for \progname{}. It explains how the software and related artifacts will be reviewed, tested, and evaluated to provide confidence that the implemented pipeline satisfies its documented requirements and is suitable for its intended use.












%%%%% 2 General Information %%%%%
\section{General Information}




% 2.1 Summary
\subsection{Summary}


\progname{} aims to automate \href{https://mne.tools/stable/documentation/cookbook.html}{the typical M/EEG workflow}. Once the pipeline configuration is set up, user can simply call pipeline methods to execute analysis steps and access results without writing additional code. For example, user defines how to process raw data, events and epochs in the configuration, then it only takes a method call to get the resulting evoked data.




% 2.2 Objectives
\subsection{Objectives}


The primary objective of this Verification and Validation (V\&V) plan is to build confidence in the correctness, reliability, and maintainability of the Eelbrain pipeline. In particular, this plan prioritizes:


\begin{itemize}
    \item \textbf{Functional correctness:} Ensuring that the implemented pipeline behaviour conforms to the functional requirements specified in the SRS.
    \item \textbf{Reliability:} Verifying that analyses produce consistent and structurally valid outputs for valid BIDS datasets.
    \item \textbf{Maintainability:} Ensuring that the implementation can accommodate likely future changes with reasonable effort.
\end{itemize}


\noindent The following objectives are considered out of scope for this V\&V plan.


\begin{itemize}
    \item \textbf{Verification of external libraries (e.g., MNE-Python):} These tools are assumed to have been independently verified by their respective development teams.
    \item \textbf{Formal mathematical verification of analysis algorithms:} The project focuses on workflow integration rather than re-validating established neuroimaging algorithms.
    \item \textbf{Extensive user studies:} Due to limited resources and time constraints, only limited usability feedback will be collected.
\end{itemize}


These scope decisions reflect practical constraints in time and resources, and allow the V\&V effort to focus on aspects of the system that are directly implemented and controlled within this project.




% 2.3 Extras
\subsection{Extras}


\begin{itemize}
    \item User manual: Provides installation steps, and small example workflows to help new users set up the environment, run the Eelbrain pipeline on sample BIDS data.
    \item Code walkthrough: Describes the main modules, data flow, and key methods in the pipeline to find potential issues.
\end{itemize}


% 2.4 Relevant Documentation
\subsection{Relevant Documentation}


\begin{itemize}
    \item Software Requirements Specification \citep{SRS}
    \item Module Guide (MG)
    \item Module Interface Specification (MIS)
\end{itemize}









%%%%% 3 Plan %%%%%
\section{Plan}


This section outlines the overall Verification and Validation (V\&V) strategy for the project. It first defines the V\&V team and their responsibilities, followed by the verification approach for the SRS, design documents, and the V\&V plan itself. The implementation verification strategy and automated testing tools are then described.


% 3.1 Verification and Validation Team
\subsection{Verification and Validation Team}


\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule 
    \textbf{Member} & \textbf{Role} & \textbf{Responsibilities} \\
\midrule
    Zhangwenchi Li & Developer & Produce the VnV Plan and refine based on feedback \\
    Dr. Spencer Smith & Course Instructor & Provides feedback on engineering principles \\
    Dr. Christian Brodbeck & Domain Expert & Provides domain-specific information on requirements and validation \\
    Xiao Shao & Reviewer & Provides feedback on V\&V plan \\
\bottomrule
\end{tabularx}




% 3.2 SRS Verification
\subsection{SRS Verification}


\begin{itemize}
    \item \textbf{Peer Review.} A classmate will review the SRS in detail using the course SRS checklist and make comments by creating issues in the project repository. The SRS will be revised to address each issue, and issues will only be closed after confirmation by the original reviewer.
    \item \textbf{Supervisor Review.} The SRS will be walked through in a scheduled meeting to familiarize the supervisor with the document structure and the purpose of each section. A checklist will be created to identify most important sections to review (e.g., goal statements, instance models, requirements). Feedback will be given by the supervisor after reading the SRS.
\end{itemize}



% 3.3 Design Verification
\subsection{Design Verification}


A classmate will review the MG and MIS document in detail using corresponding checklists in course repo and make comments by creating issues in the project repository.




% 3.4 Verification and Validation Plan Verification
\subsection{Verification and Validation Plan Verification}


A classmate will review the VnV Plan in detail using checklist in course repo and make comments by creating issues in the project repository.




% 3.5 Implementation Verification
\subsection{Implementation Verification}


Unit testing planned in \autoref{unit_test_description} will be developed as the major method for implementation verification. A code walkthrough will be performed to inspect the most critical modules.




% 3.6 Automated Testing and Verification Tools
\subsection{Automated Testing and Verification Tools}


\begin{itemize}
    \item \textbf{GitHub Actions}: Automates CI/CD workflows to run tests, checks, and deployments on every push or pull request.
    \item \textbf{pytest}: A Python testing framework used to write and execute unit and integration tests efficiently. The written tests will be integrated and executed in GitHub Actions.
    \item \textbf{flake8}: A linting tool that checks Python code for style issues, syntax errors, and adherence to coding standards.
    \item \textbf{Codecov}: A code coverage reporting tool that measures how much of the codebase is tested and produces coverage reports.
\end{itemize}




% 3.7 Software Validation
\subsection{Software Validation}


Out of scope for this project.












%%%%% 4 System Tests %%%%%
\section{System Tests}


This section defines the system-level tests derived from the Software Requirements Specification (SRS). It first presents test cases for functional requirements, followed by tests addressing nonfunctional requirements such as usability, maintainability, and portability. Finally, a traceability matrix is provided to explicitly link requirements to their corresponding test cases. All system tests will use the \href{https://mne.tools/stable/documentation/datasets.html#sample}{MNE sample dataset} as input data.



% 4.1 Tests for Functional Requirements
\subsection{Tests for Functional Requirements}


The following system-level test cases are derived from the functional requirements (R1--R5) identified in the SRS. For each functional requirement, at least one corresponding test case (FR-01--FR-04) is defined, specifying initial state, inputs, expected outputs, and pass/fail criteria. This ensures that the implemented Eelbrain pipeline behaviour can be systematically checked against the required functionality, and that there is clear traceability from the SRS requirements to the verification activities.

		
\paragraph{BIDS Path Templating}


\begin{enumerate}


\item{FR-01\\}
    Control: Automatic \\
    Initial State: Eelbrain pipeline is initialized. \\
    Input:
        \begin{itemize}
            \item MNE sample dataset containing multiple file paths;
            \item Methods calls on pipeline to access file paths and load files.
        \end{itemize}
    Output: Eelbrain pipeline successfully loads all correct file paths or handles incorrect ones with informative errors. \\
    Test Case Derivation: From MNE sample data. \\
    How test will be performed: Call pipeline methods to get paths or load files. Assertions will check that
        \begin{itemize}
            \item All file paths are correctly constructed from file templates in the valid case;
            \item All files are successfully discovered in the valid case;
            \item An appropriate exception or error message is raised in the invalid case, explicitly identifying missing files.
        \end{itemize}


\end{enumerate}


\begin{tabularx}{\textwidth}{p{2cm}p{2cm}p{4.5cm}X}
\toprule 
    \textbf{Test Case ID} & \textbf{File Type} & \textbf{Path Invoked} & \textbf{Expected Behaviour} \\
\midrule
    FR-01-1 & Raw MEG data & sub-01/ses-01/meg/ sub-01\_ses-01\_task-auditory\_meg.fif & Pipeline method returns a loaded MNE Raw object with correct subject, session and task information. \\

    FR-01-2 & Raw MEG data & sub-01/meg/sub-01\_task-auditory\_meg.fif & Pipeline raises FileNotFoundError with full expected path. \\

    FR-01-3 & Channels file & sub-01/ses-01/meg/sub-01\_ses-01\_task-auditory\_channels.tsv & Pipeline method can read channels file correctly, returning an empty bad channels list. \\

    FR-01-4 & FreeSurfer MRI & {root}/derivatives/ freesurfer/sub-01/bem/outer\_skull.surf & Pipeline method returns the bem model. \\
\bottomrule
\end{tabularx}


\paragraph{Automatic parameter inference from dataset}


\begin{enumerate}


\item{FR-02\\}
    Control: Automatic \\
    Initial State: Eelbrain pipeline is initialized. \\
    Input: 
        \begin{itemize}
            \item A fake dataset with multiple subjects, sessions and tasks created by duplicating MNE sample dataset;
            \item A minimal pipeline configuration file with BIDS entities omitted (e.g., subject/session/task).
        \end{itemize}
    Output: BIDS entities are properly inferred from the dataset. \\
    Test Case Derivation: From MNE sample data. \\
    How test will be performed: Access pipeline attributes related to BIDS entities. Assertions will check that the set of correct BIDS entities and inferred values are identical.


\end{enumerate}


\begin{tabularx}{\textwidth}{p{2cm}p{2cm}p{4.5cm}X}
\toprule 
    \textbf{Test Case ID} & \textbf{BIDS Entity} & \textbf{Example Dataset Structure} & \textbf{Expected Inferred Value / Behaviour} \\
\midrule
    FR-02-1 & Subject & Dataset contains sub-01, sub-02, sub-03 directories & The pipeline subjects list is \texttt{["01", "02", "03"]} \\

    FR-02-2 & Session & Dataset contains ses-01, ses-02 directories under each subject directory & The pipeline sessions list is \texttt{["01", "02"]} \\

    FR-02-3 & Task & Dataset contains task-sample1, task-sample2 directories under each session directory & The pipeline tasks list is \texttt{["sample1", "sample2"]}. \\
\bottomrule
\end{tabularx}


\paragraph{Support for sensor-space analysis}


\begin{enumerate}


\item{FR-03\\}
    Control: Automatic \\
    Initial State: Eelbrain pipeline initialized. \\
    Input:
        \begin{itemize}
            \item sub-01\_task-auditory\_meg.fif in MNE sample dataset;
            \item A pipeline configuration includes sensor-space processing parameters (preprocessing steps, epoch definitions, etc.);
            \item Methods calls on pipeline to execute sensor-space analysis steps and load processed data.
        \end{itemize}
    Output: The pipeline successfully processes sensor-space data with correct dimensions. \\
    Test Case Derivation: From MNE sample data. \\
    How test will be performed: Assertions will check that
    \begin{itemize}
        \item The returned sensor-space data are in correct MNE-Python data structures (e.g., Epochs objects);
        \item The dimensions of processed sensor-space data match expected values (the number of channels, time points, or epochs).
    \end{itemize}
    


\end{enumerate}


\begin{tabularx}{\textwidth}{p{2.5cm}p{5cm}X}
\toprule 
    \textbf{Test Case ID} & \textbf{Processing Stage} & \textbf{Anticipated Result} \\
\midrule

    FR-03-1 & Maxwell filtering & Returned raw object maintains 376 channels and 600.614 Hz sampling rate. No NaN or infinite values in data array. \\

    FR-03-2 & Epochs extraction & Epochs object with shape (280--300 epochs, 376 channels, 421 time points). Epoch rejection rate less than 15\%. \\

    FR-03-3 & Evoked response computation & Evoked object averaged across trials with shape (376 channels, 421 time points). Peak amplitude 250--350 fT for auditory sensors (channels MEG14xx). \\
\bottomrule
\end{tabularx}


\paragraph{Support for source-space analysis}


\begin{enumerate}


\item{FR-04\\}
    Control: Automatic \\
    Initial State: Eelbrain pipeline initialized and sensor-space processing completed. \\
    Input:
        \begin{itemize}
            \item A valid BIDS dataset with Freesurfer MRI and trans files;
            \item Methods calls on pipeline to execute source-space analysis steps and load STC (source time course) data.
        \end{itemize}
    Output: The pipeline successfully computes source-space data with correct properties. \\
    Test Case Derivation: From MNE sample data. \\
    How test will be performed: Run the source localization workflow. Assertions will check that 
    \begin{itemize}
        \item The returned source-space data is in an MNE-Python SourceEstimate object;
        \item The returned source-space data has a shape of 7498 vertices and 421 time points; its peak amplitude in right auditory cortex is 15--25 dSPM units.
    \end{itemize}
    


\end{enumerate}




% 4.2 Tests for Nonfunctional Requirements
\subsection{Tests for Nonfunctional Requirements}





\subsubsection{Usability}


\paragraph{Usability survey acceptance test}


\begin{enumerate}


\item{NFR-01\\}
    Type: Manual \\
    Initial State: An alpha version of Eelbrain pipeline is available; a user group consisting of neuroscience researchers is selected. \\
    Input/Condition: Participants complete a short analysis task (load raw files, compute evoked response and access results) and then complete a usability survey questionnaire. \\
    Output/Result: At least 80\% of respondents rate the software as easy to use. \\
    How test will be performed: Collect questionnaire results; compute the percentage of "easy to use" responses and verify if it is larger than 80\%.


\end{enumerate}




\subsubsection{Maintainability}


\paragraph{Likely-change effort threshold test}


\begin{enumerate}


\item{NFR-02\\}
    Type: Dynamic, Manual \\
    Initial State: Baseline development time is recorded for the original implementation; codebase is under version control. \\
    Input/Condition: Select one likely change (e.g., adding support for one more BIDS entity or one more preprocessing operation) and implement it by a developer with relevant domain knowledge. \\
    Output/Result: The measured effort to implement the chosen likely change is less than 0.5 of the original development time. \\
    How test will be performed: Track engineering time using commits; compare total effort against the recorded baseline and confirm it satisfies the threshold.


\end{enumerate}


\subsubsection{Portability}


\paragraph{Cross-platform installation and execution test}


\begin{enumerate}


\item{NFR-03\\}
    Type: Manual \\		
    Initial State: Clean Windows, macOS, and Linux environments prepared. \\
    Input/Condition: Install Eelbrain pipeline and run developed pytest scripts on each OS using the MNE sample dataset. \\
    Output/Result: The pipeline installs and runs successfully on Windows, macOS, and Linux with consistent results for the workflow. \\
    How test will be performed: Create different workflows for each OS on Github Actions.


\end{enumerate}




% 4.3 Traceability Between Test Cases and Requirements
\subsection{Traceability Between Test Cases and Requirements}


\begin{table}[h]
\centering
\begin{tabularx}{0.6\textwidth}{p{3cm}X}
\toprule
    \textbf{Requirements} & \textbf{Test Cases} \\
\midrule
    R1 & FR-01 \\
    R2 & FR-02 \\
    R3 & FR-03, FR-05 \\
    R4 & FR-04, FR-05 \\
    NFR1 & NFR-01 \\
    NFR2 & NFR-02 \\
    NFR3 & NFR-03 \\
\bottomrule
\end{tabularx}
\end{table}




\newpage








%%%%% 5 Unit Test Description %%%%%
\section{Unit Test Description} \label{unit_test_description}




% 5.1 Unit Testing Scope
\subsection{Unit Testing Scope}


% scope, priority




% 5.2 Tests for Functional Requirements
\subsection{Tests for Functional Requirements}




\subsubsection{Module 1}


\begin{enumerate}


\item{test-id1\\}
    Type: Automatic \\
    Initial State: \\
    Input: \\
    Output: \\
    Test Case Derivation: \\
    How test will be performed:


\end{enumerate}




% 5.3 Tests for Nonfunctional Requirements
\subsection{Tests for Nonfunctional Requirements}




% 5.4 Traceability Between Test Cases and Modules
\subsection{Traceability Between Test Cases and Modules}




\newpage








\bibliographystyle{plainnat}
\bibliography{../../refs/References}




\newpage








% \section{Appendix}




% \subsection{Symbolic Parameters}




% \subsection{Usability Survey Questions?}








\end{document}